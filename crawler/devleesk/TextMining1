tf - term frequency 

단어를 표현하는 가장 쉬운 방법 -> 이진(binary) 표현법

가 = 		[1,0,0,...,0]
가가 = 		[0,1,0,...,0]
가가대소 = 	[0,0,1,...,0]
...
d자 =	 	[0,0,0,...,1]

백터에 단어 표현 가는 0번째, 가가는 1번째
이렇게 표현한 벡터는 one-hot vector 부르며, 이런 방식의 인코딩을 1-of-V 코딩이라 함
=> but 이진 표현법은 단어 간 유사도를 정의할 수 없다.

같은 철학으로 문서를 표현한 것
bag of words(BOW)

단어가 문서에 존재한다/안한다(= term existance)
document1 = ["가 가가"]
document1 = [1,1,0,...,0]

단어가 문서에 n번 존재한다(term frequency[tf])
document2 = ["가 가 가 가가"]
document2 = [3,1,0,...,0]

단어에 중요도를 할당하고 문서에 등장한 단어 중요도의 가중합을 구한다.
ex : term frequency - inverse document frequency(TF-IDF)

=> but 공간의 차원이 너무 커 문서 간 유사도 지표의 효과 감소
1 공간의 차원이 커질수록(문서들이 많아지고, 문서에 나오는 단어가 다양해질수록) 데이터의 대부분은 공간의 꼭지에 분포(특정 단어들로 쏠림)
2 문서 간 거리의 편차가 매우 적어짐
3 문서 간 유사도 비슷함

워드넷 같은 텍소노미 활요
워드넷 = X is-a Y와 같은 상위어(hypernym) 관계 등을 정의하는 방향성 비순환 그래프(DAG:directed acyclic graph) ex: 사과는 과일
워드넷 = 부산대에서 개발 = 권대건에게 문의 ㅋㅋㅋㅋㅋㅋㅋㅋ
워드넷의 단점
1 전문 도메인 용어x
2 신조어 확보 필요
3 기반이 되는 NLTK는 한국어 지원 안됨

유유상종(친구를 보면, 그 사람을 안다)=끼리끼리 논다인 줄 알았는뎀,
=> 단어의 주변을 보면, 그 단어를 안다.

다시 말해, 단어의 의미는 해당 단어의 문맥(context)이 담고 있다.
ex 1 영희가 철수에게 미안하다고 [사과]하면서
ex 2 나무에서 갓 딴 맛있는 [사과]를 주었습니다.

문맥(context) : 정해진 구간(window) 또는 문장/문서 내의 단어들

Co-occurrence : 두 단어가 정해진 구간 내에서 동시에 등장함
document[
	["나는", "파이썬", "이", "좋다"],
	["나는", "R", "이", "좋다"]
]
1. term-document matrix : 한 문서에 같이 등장하면 비슷한 단어
- computation이 문서의 개수에 비례
ex : x_td=[
	[1,1], # 나는
	[1,0], # 파이썬
	[0,1], # R
	[1,1], # 이
	[1,1] # 좋다
]

2. term-term matrix : 단어가 문맥 내에 같이 존재하면 비슷한 단어
- 문맥의 길이가 짧을수록 syntactic, 길수록 semantic 정보를 포함
- 앞뒤로 단어를 두 개씩 보는 경우(문맥의 길이 == 5)
ex : x_tt=[
	[0,1,1,2,0], #나는
	[1,0,0,1,1], #파이썬
	[1,0,0,1,1], #R
	[2,1,1,0,2], #이
	[0,1,1,2,0], #좋다
## 이 부분 이해 잘 안감...

Co_occurrence matrix를 있는 그대로 이용-> 단어간 유사도 구할 수 있다.
- but, 값들이 너무 skewed 되어 있다.(즉, 빈도 높은 단어와 낮은 단어의 격차가 큼)
- 정보성 낮은 단어 때문에 discriminative 하지 않음 (ex : list('은는이가'))

- 유사도 정보가 담긴 단어 벡터를 구하는 더 나은 방법
=> Neural embeddings
idea : 문맥에 있는 단어 예측
 - ex : ["나는", "파이썬", "이", "좋다"] 다음에 뭐가 나올까? ex:"!","?","."
학습할 때 뉴럴넷 사용
Neural network language model(NNLM)
 - word 2 vec (워드 투 벡터)
 - T.Mikolov의 word2vec - neural embedding
 - 각종 계산 트릭을 적용 -> 컴퓨테이션이 빨라짐(n일 -> m 시간)

doc 2 ver (문서에 대한 neural embedding)
idea : 문서(또는 문단) 벡터를 마치 단어인 양 학습시키자!
장점 - 
차원이 |V|에 비해 훨씬 적어짐
단어 벡터와 같은 공간에 문서를 전사할 수 있다.
ex : Term frequency => 문서 벡터의 크기가 단어의 수|V|와 같음, 원소들은 양의 정수
document = [
	["왕자","가","공주","를","좋아한다"],
	["공주","가","왕자","를","좋아한다"],
	["시녀","가","왕자","를","싫어한다"],
	["공주","가","시녀","를","싫어한다"],
	["시녀","가","왕자","를","독살한다"],
	["시녀","가","공주","를","독살한다"]
]
words = set(sum(documents,[]))
print(words)

def term_frequency(document):
	# do something else (뒤에서 Gensim이 해줄 것!)
	return vector
vectors = [term_frequency(doc) for doc in documents]
print(vectors)

요약
- 단어의 문맥을 이용해서 단어를 표현
- 텍스트의 의미를 벡터로 표현하는 방법:
Sparse, long vectors
- 단어 : 1-hot-vectors
- 문서 : bag-of-words(term existance,TF,TF-IDF 등)

Dense, short vectors
- 단어 : word2vec
- 문서 : doc2vec


예제 위주 => 데이터는 네이버 영화 리뷰로 정함
git url = http://github.com/e9t/nsmc/

순서
1. KoNLPy로 전처리
2. NLTK 탐색
3. Bag-of-words(term existence)로 문서 표현하고 classify
4. Gensim으로 doc2vec으로 문서 표현하고 classify

데이터 읽기 - 알아서 읽으셈
data = open(file_name, 'r')

데이터 확인 - row로 확인하넹
print(len(data))
print(len(data[0]))

KoNLPy : 쉽고 간결한 한국어 정보처리 파이썬 패키지 - Lucy님이 만든듯
설치방법 : http://konlpy.org/ko/v0.4.3/install/
관련 논문 : http://dmlab.snu.ac.kr/~lucypark/docs/2014-10-10-hclt.pdf

형태소 토크나이징
from konlpy.tag import twitter
pos_tagger = Twitter()

def tokenize(doc):
	return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]
train_docs = [(tokenize(row[1]), row[2]) for row in train_data]
train_docs = [(tokenize(row[1]), row[2]) for row in test_data]

from pprint import pprint
pprint(train_docs[0])

1. data preprocessing
- 데이터가 많아서, 어절 단위로도 분석하다면, 형태소 필요 없지만, 그렇지 않으니 형태소 나눔
- 최근 히트친 시인뉴럴은 음절 단위 분석
- 토큰의 단위는 선택 문제
- 품사 태그 부착도 선택의 문제
- 품사 태깅해두면 동음이의어를 구분 가능한 장점

2. data exploration 
- training data의 token 모으기

tokens = [t for d in train_docs for t in d[0]]
print(len(tokens))
-> nltk.Text() : Exploration 편함
-> 단일 document를 분석할 때는 풍부한 기능

import nltk
text = nltk.Text(tokens, name='NMSC')
print(text)

print(len(text.tokens))
print(len(set(text.tokens)))
print(text.vocab().most_common(10))

-plot tokens by frequency
text.plot(50)
한글 깨질시 문서 확인

Collocations(연어) : 인접하게 빈번하게 등장하는 단어 (ex : "text" + "mining")
text.collocations() # 시간 오래 걸릴 수 있다.

3. Sentiment classification with term-existance
- term이 문서에 존재하는지의 유무에 따라 분류
selected_words = [f[0] form f in text.vocab().most_common(2000)]
# 최빈도 단어 2000개

def term_exists(doc):
	return {'exists({})'.format(word) : (word in set(doc)) for word in selected_words}

train_docs = train_docs[:1000]

train_xy = [(term_exists(d),c) for d, c in train_docs]
test_xy = [(term_exists(d),c) for d, c in test_docs]

try this :
최빈도 단어들은 의미가 없을 수 있다. << 생각지도 못한 의미
=> 최상위 50개 제거 후 분석
TF, TF-IDF 성능
=> 최적화 sparse matrix or scikit-learn[TfidfVectorizer()] 사용

next, 분류!!!
NLTK에서 제공하는
1. Naive Bayes Classifiers
2. Decision Tree Classifiers
3. Maximum Entropy Classifiers
등등 -> 다 해보자 우리는

Naive Bayes classifier 예제
classifier = nltk.NaiveBayesClassifier.train(train_xy)
print(nltk.classify.accuracy(classifier, test_xy))
classfier.show_most_informative_features(10)

4. Sentiment classification with doc2ver (feat. Gensim)
- doc2vec로 리뷰 긍/부정 분류
- Radim Rehureck(거제도에 산다고함 ㅋㅋㅋ) - gensim 버전업 빠름
- doc2vec이 요구하는 형태로 데이터를 바꿔줘야 하기 때문에 약간 복잡
from gensim.models.doc2vec import TaggedDocument
or
from collections import namedtuple

TaggedDocument = namedtuple('TaggedDocument', 'words tags')

tagged_train_docs = [TaggedDocument(d, [c]) for d, c in train_docs]
tagged_test_docs = [TaggedDocument(d, [c]) for d, c in test_docs]

from gensim.models import doc2vec

doc_vectorizer = doc2vec.Doc2Vec(size=300, alpha=0.025, min_alpha=0.025, seed=1234)
doc_vectorizer.build_vocab(tagged_train_docs)

for epoch in range(10):
	doc_vectorizer.train(tagged_train_docs)
	doc_vectorizer.alpha = 0.002
	doc_vectorizer.min_alpha = doc_vectorizer.alpha

try this :
vector size, epoch 수치 바꿔보기

학습 확인
pprint(doc_vectorizer.most_similar('공포/Noun'))
pprint(doc_vectorizer.most_similar('ㅋㅋ/KoreanParticle'))
pprint(doc_vectorizer.most_similar(positive=['여자/Noun', '왕/Noun'], negative=['남자/Noun']))

# 결과 이상함
text.concordance('왕/Noun', lines=10)

# 다의어 구분
train_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_train_docs]
train_y = [doc.tags[0] for doc in tagged_train_docs]
len(train_x)

test_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_test_docs]
test_y = pdoc.tags[0] for doc in tagged_test_docs]
len(test_x)
len(test_x[0])

scikit-learn에서 LogisticRegression() 사용
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state=1234)
classifier.fit(train_x, train_y)
classifier.score(test_x, test_y)


출처 : https://www.slideshare.net/lucypark/nltk-gensim